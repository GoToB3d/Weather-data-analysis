{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (2.32.3)\n",
      "Collecting bs4\n",
      "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
      "Collecting lxml\n",
      "  Downloading lxml-5.3.0-cp312-cp312-macosx_10_9_universal2.whl.metadata (3.8 kB)\n",
      "Collecting xlsxwriter\n",
      "  Downloading XlsxWriter-3.2.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests) (2024.2.2)\n",
      "Requirement already satisfied: beautifulsoup4 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from bs4) (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from beautifulsoup4->bs4) (2.5)\n",
      "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
      "Downloading lxml-5.3.0-cp312-cp312-macosx_10_9_universal2.whl (8.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading XlsxWriter-3.2.0-py3-none-any.whl (159 kB)\n",
      "Installing collected packages: xlsxwriter, lxml, bs4\n",
      "Successfully installed bs4-0.0.2 lxml-5.3.0 xlsxwriter-3.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install requests bs4 lxml xlsxwriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests.compat import urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import xlsxwriter\n",
    "import time\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished scraping data for 2011...\n",
      "---------------------Scraping Complete----------------\n"
     ]
    }
   ],
   "source": [
    "def CleanData(InfoString):\n",
    "    InfoString = InfoString.replace('\\n', '',)  \n",
    "    InfoString = InfoString.replace('\\r', '',)  \n",
    "    InfoString = InfoString.replace(' ', '',)\n",
    "    InfoString = InfoString.replace('年', '-', 1)  \n",
    "    InfoString = InfoString.replace('月', '-', 1)\n",
    "    InfoString = InfoString.replace('日', '', 1)\n",
    "    return InfoString\n",
    "\n",
    "def ExtractBJWeather():\n",
    "    # Scrape Beijing weather data\n",
    "    output_file = 'Beijing_Weather_Data.csv'\n",
    "    with open(output_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Date', 'Weather Condition', 'Temperature', 'Wind Force and Direction'])  # CSV headers\n",
    "\n",
    "        url = 'http://www.tianqihoubao.com/lishi/beijing.html'\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.106 Safari/537.36',\n",
    "            'Connection': 'close'\n",
    "        }\n",
    "        html = requests.get(url, headers=headers)\n",
    "        html.encoding = 'gb2312'  # Correctly set the encoding to match the website\n",
    "        bsObj = BeautifulSoup(html.text, 'lxml')  # Use text instead of content with the correct encoding\n",
    "\n",
    "        year_weather_data = bsObj.find_all('div', class_=\"box pcity\")  # Weather data for all years\n",
    "        year = 2010\n",
    "        for quarter_data in year_weather_data:  # Iterate through quarterly data\n",
    "            year += 1\n",
    "            if year <= 2011:  # Restrict to data up to 2020\n",
    "                quarter_list = quarter_data.find_all('ul')  # Quarterly data\n",
    "                for month_data in quarter_list:  # Iterate through months in a quarter\n",
    "                    month_links = month_data.find_all('a')  # Extract links for each month\n",
    "                    for link in month_links:\n",
    "                        month_url = urljoin('http://www.tianqihoubao.com', link['href'])\n",
    "                        try:\n",
    "                            month_html = requests.get(month_url, headers=headers)\n",
    "                            month_html.encoding = 'gb2312'  # Correct encoding\n",
    "                            month_bsObj = BeautifulSoup(month_html.text, 'lxml')\n",
    "                            month_rows = month_bsObj.find_all('tr')  # Rows of monthly data\n",
    "                            for i, day_row in enumerate(month_rows):\n",
    "                                if i == 0:  # Skip the header row\n",
    "                                    continue\n",
    "                                day_data = [CleanData(td.get_text()) for td in day_row.find_all('td')]\n",
    "                                if len(day_data) == 4:\n",
    "                                    writer.writerow(day_data)  # Write data to CSV file\n",
    "                        except Exception as e:  # Handle request rate limit\n",
    "                            print(\"Request rate too high, sleeping for 5 seconds...\")\n",
    "                            time.sleep(5)\n",
    "                            print(\"Retrying...\")\n",
    "                            continue\n",
    "                print(f'Finished scraping data for {year}...')\n",
    "            else:\n",
    "                break\n",
    "    print('---------------------Scraping Complete----------------')\n",
    "    return\n",
    "\n",
    "\n",
    "ExtractBJWeather()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished scraping data for beijing:2011...\n",
      "Finished scraping data for shanghai:2011...\n",
      "Finished scraping data for guangzhou:2011...\n",
      "Weather data for all cities has been written to Weather_Data_All_Cities.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "def CleanData(InfoString):\n",
    "    \"\"\"Clean the data by removing unwanted characters\"\"\"\n",
    "    InfoString = InfoString.replace('\\n', '',)  \n",
    "    InfoString = InfoString.replace('\\r', '',)  \n",
    "    InfoString = InfoString.replace(' ', '',)\n",
    "    InfoString = InfoString.replace('年', '-', 1)  \n",
    "    InfoString = InfoString.replace('月', '-', 1)\n",
    "    InfoString = InfoString.replace('日', '', 1)\n",
    "    return InfoString\n",
    "\n",
    "def ExtractWeatherForCity(city):\n",
    "    \"\"\"Scrape weather data for the specified city and return a list of records\"\"\"\n",
    "    url = f'http://www.tianqihoubao.com/lishi/{city}.html'\n",
    "    \n",
    "    data_records = []  # List to hold the weather data\n",
    "\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.106 Safari/537.36',\n",
    "        'Connection': 'close'\n",
    "    }\n",
    "    html = requests.get(url, headers=headers)\n",
    "    html.encoding = 'gb2312'  # Correctly set the encoding to match the website\n",
    "    bsObj = BeautifulSoup(html.text, 'lxml')\n",
    "\n",
    "    year_weather_data = bsObj.find_all('div', class_=\"box pcity\")  # Weather data for all years\n",
    "    year = 2010\n",
    "    for quarter_data in year_weather_data:  # Iterate through quarterly data\n",
    "        year += 1\n",
    "        if year <= 2011:  # Restrict to data up to 2020\n",
    "            quarter_list = quarter_data.find_all('ul')  # Quarterly data\n",
    "            for month_data in quarter_list:  # Iterate through months in a quarter\n",
    "                month_links = month_data.find_all('a')  # Extract links for each month\n",
    "                for link in month_links:\n",
    "                    month_url = urljoin('http://www.tianqihoubao.com', link['href'])\n",
    "                    try:\n",
    "                        month_html = requests.get(month_url, headers=headers)\n",
    "                        month_html.encoding = 'gb2312'  # Correct encoding\n",
    "                        month_bsObj = BeautifulSoup(month_html.text, 'lxml')\n",
    "                        month_rows = month_bsObj.find_all('tr')  # Rows of monthly data\n",
    "                        for i, day_row in enumerate(month_rows):\n",
    "                            if i == 0:  # Skip the header row\n",
    "                                continue\n",
    "                            day_data = [CleanData(td.get_text()) for td in day_row.find_all('td')]\n",
    "                            if len(day_data) == 4:\n",
    "                                day_data.insert(1, city)  # Insert city name at the second column\n",
    "                                data_records.append(day_data)  # Add to the list of records\n",
    "                    except Exception as e:  # Handle request rate limit\n",
    "                        print(f\"Error scraping {city} data for month: {link['href']}, retrying...\")\n",
    "                        time.sleep(5)\n",
    "                        continue\n",
    "            print(f'Finished scraping data for {city}:{year}...')\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return data_records\n",
    "\n",
    "def WriteWeatherDataToCSV(city_list):\n",
    "    \"\"\"Scrape weather data for all cities and write them to a CSV file\"\"\"\n",
    "    all_data = []  # List to hold data from all cities\n",
    "\n",
    "    # Loop over each city in the city_list and scrape its weather data\n",
    "    for city in city_list:\n",
    "        city_data = ExtractWeatherForCity(city)\n",
    "        all_data.extend(city_data)  # Append the data of this city to the overall list\n",
    "\n",
    "    # Write all collected data to a CSV file at once\n",
    "    output_file = 'Weather_Data_All_Cities.csv'\n",
    "    with open(output_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Date', 'City', 'Weather Condition', 'Temperature', 'Wind Force and Direction'])  # CSV headers\n",
    "        writer.writerows(all_data)  # Write all records at once\n",
    "\n",
    "    print(f'Weather data for all cities has been written to {output_file}')\n",
    "\n",
    "city_list = ['beijing', 'shanghai', 'guangzhou']\n",
    "WriteWeatherDataToCSV(city_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
