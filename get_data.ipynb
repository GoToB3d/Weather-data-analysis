{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (2.32.3)\n",
      "Collecting bs4\n",
      "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
      "Collecting lxml\n",
      "  Downloading lxml-5.3.0-cp312-cp312-macosx_10_9_universal2.whl.metadata (3.8 kB)\n",
      "Collecting xlsxwriter\n",
      "  Downloading XlsxWriter-3.2.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests) (2024.2.2)\n",
      "Requirement already satisfied: beautifulsoup4 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from bs4) (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from beautifulsoup4->bs4) (2.5)\n",
      "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
      "Downloading lxml-5.3.0-cp312-cp312-macosx_10_9_universal2.whl (8.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading XlsxWriter-3.2.0-py3-none-any.whl (159 kB)\n",
      "Installing collected packages: xlsxwriter, lxml, bs4\n",
      "Successfully installed bs4-0.0.2 lxml-5.3.0 xlsxwriter-3.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install requests bs4 lxml xlsxwriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from urllib.parse import urljoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished scraping data for 2011...\n",
      "Finished scraping data for 2012...\n",
      "Finished scraping data for 2013...\n",
      "Finished scraping data for 2014...\n",
      "Finished scraping data for 2015...\n",
      "Finished scraping data for 2016...\n",
      "Finished scraping data for 2017...\n",
      "Finished scraping data for 2018...\n",
      "Finished scraping data for 2019...\n",
      "Finished scraping data for 2020...\n",
      "Finished scraping data for 2021...\n",
      "Scraping data for beijing completed!\n",
      "Finished scraping data for 2011...\n",
      "Finished scraping data for 2012...\n",
      "Finished scraping data for 2013...\n",
      "Finished scraping data for 2014...\n",
      "Finished scraping data for 2015...\n",
      "Finished scraping data for 2016...\n",
      "Finished scraping data for 2017...\n",
      "Finished scraping data for 2018...\n",
      "Finished scraping data for 2019...\n",
      "Finished scraping data for 2020...\n",
      "Finished scraping data for 2021...\n",
      "Scraping data for shanghai completed!\n",
      "Finished scraping data for 2011...\n",
      "Finished scraping data for 2012...\n",
      "Finished scraping data for 2013...\n",
      "Finished scraping data for 2014...\n",
      "Finished scraping data for 2015...\n",
      "Finished scraping data for 2016...\n",
      "Finished scraping data for 2017...\n",
      "Finished scraping data for 2018...\n",
      "Finished scraping data for 2019...\n",
      "Finished scraping data for 2020...\n",
      "Finished scraping data for 2021...\n",
      "Scraping data for lasa completed!\n",
      "Finished scraping data for 2011...\n",
      "Finished scraping data for 2012...\n",
      "Finished scraping data for 2013...\n",
      "Finished scraping data for 2014...\n",
      "Finished scraping data for 2015...\n",
      "Finished scraping data for 2016...\n",
      "Finished scraping data for 2017...\n",
      "Finished scraping data for 2018...\n",
      "Finished scraping data for 2019...\n",
      "Finished scraping data for 2020...\n",
      "Finished scraping data for 2021...\n",
      "Scraping data for haikou completed!\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "def CleanData(InfoString):\n",
    "    \"\"\"Clean the data by removing unwanted characters\"\"\"\n",
    "    InfoString = InfoString.replace('\\n', '',)  \n",
    "    InfoString = InfoString.replace('\\r', '',)  \n",
    "    InfoString = InfoString.replace(' ', '',)\n",
    "    InfoString = InfoString.replace('年', '-', 1)  \n",
    "    InfoString = InfoString.replace('月', '-', 1)\n",
    "    InfoString = InfoString.replace('日', '', 1)\n",
    "    return InfoString\n",
    "\n",
    "def ExtractWeatherForCity(city):\n",
    "    \"\"\"Scrape weather data for the specified city and append to the same CSV file\"\"\"\n",
    "    # Define the base URL for the weather history page for different cities\n",
    "    url = f'http://www.tianqihoubao.com/lishi/{city}.html'\n",
    "    \n",
    "    # Open the same CSV file to append data from different cities\n",
    "    output_file = 'Weather_Data_All_Cities.csv'\n",
    "    with open(output_file, mode='a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        \n",
    "        # If it's the first city, write the header\n",
    "        if file.tell() == 0:  # Check if file is empty, i.e., first write\n",
    "            writer.writerow(['Date', 'City', 'Weather Condition', 'Temperature', 'Wind Force and Direction'])  # CSV headers\n",
    "        \n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.106 Safari/537.36',\n",
    "            'Connection': 'close'\n",
    "        }\n",
    "        html = requests.get(url, headers=headers)\n",
    "        html.encoding = 'gb2312'  # Correctly set the encoding to match the website\n",
    "        bsObj = BeautifulSoup(html.text, 'lxml')\n",
    "\n",
    "        # Find the weather data for the city\n",
    "        year_weather_data = bsObj.find_all('div', class_=\"box pcity\")  # Weather data for all years\n",
    "        year = 2010\n",
    "        for quarter_data in year_weather_data:  # Iterate through quarterly data\n",
    "            year += 1\n",
    "            if year <= 2021:  # Restrict to data up to 2020\n",
    "                quarter_list = quarter_data.find_all('ul')  # Quarterly data\n",
    "                for month_data in quarter_list:  # Iterate through months in a quarter\n",
    "                    month_links = month_data.find_all('a')  # Extract links for each month\n",
    "                    for link in month_links:\n",
    "                        month_url = urljoin('http://www.tianqihoubao.com', link['href'])\n",
    "                        try:\n",
    "                            month_html = requests.get(month_url, headers=headers)\n",
    "                            month_html.encoding = 'gb2312'  # Correct encoding\n",
    "                            month_bsObj = BeautifulSoup(month_html.text, 'lxml')\n",
    "                            month_rows = month_bsObj.find_all('tr')  # Rows of monthly data\n",
    "                            for i, day_row in enumerate(month_rows):\n",
    "                                if i == 0:  # Skip the header row\n",
    "                                    continue\n",
    "                                day_data = [CleanData(td.get_text()) for td in day_row.find_all('td')]\n",
    "                                if len(day_data) == 4:\n",
    "                                    day_data.insert(1, city)  # Insert city name at the second column\n",
    "                                    writer.writerow(day_data)  # Write data to CSV file\n",
    "                        except Exception as e:  # Handle request rate limit\n",
    "                            print(f\"Error scraping {city} data for month: {link['href']}, retrying...\")\n",
    "                            time.sleep(5)\n",
    "                            continue\n",
    "                print(f'Finished scraping data for {year}...')\n",
    "            else:\n",
    "                break\n",
    "    print(f'Scraping data for {city} completed!')\n",
    "    return\n",
    "\n",
    "# Example: Scrape weather data for Beijing, Shanghai, and Guangzhou\n",
    "ExtractWeatherForCity('beijing')\n",
    "ExtractWeatherForCity('shanghai')\n",
    "ExtractWeatherForCity('lasa')\n",
    "ExtractWeatherForCity('haikou')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
