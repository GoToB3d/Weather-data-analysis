{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (2.32.3)\n",
      "Collecting bs4\n",
      "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
      "Collecting lxml\n",
      "  Downloading lxml-5.3.0-cp312-cp312-macosx_10_9_universal2.whl.metadata (3.8 kB)\n",
      "Collecting xlsxwriter\n",
      "  Downloading XlsxWriter-3.2.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests) (2024.2.2)\n",
      "Requirement already satisfied: beautifulsoup4 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from bs4) (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from beautifulsoup4->bs4) (2.5)\n",
      "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
      "Downloading lxml-5.3.0-cp312-cp312-macosx_10_9_universal2.whl (8.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading XlsxWriter-3.2.0-py3-none-any.whl (159 kB)\n",
      "Installing collected packages: xlsxwriter, lxml, bs4\n",
      "Successfully installed bs4-0.0.2 lxml-5.3.0 xlsxwriter-3.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install requests bs4 lxml xlsxwriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests.compat import urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import xlsxwriter\n",
    "import time\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...爬完 2011 年...\n",
      "...爬完 2012 年...\n",
      "...爬完 2013 年...\n",
      "requests speed so high,need sleep!\n",
      "continue...\n"
     ]
    }
   ],
   "source": [
    "def CleanData(InfoString):\n",
    "    InfoString = InfoString.replace('\\n', '',)  \n",
    "    InfoString = InfoString.replace('\\r', '',)  \n",
    "    InfoString = InfoString.replace(' ', '',)\n",
    "    InfoString = InfoString.replace('年', '-', 1)  \n",
    "    InfoString = InfoString.replace('月', '-', 1)\n",
    "    InfoString = InfoString.replace('日', '', 1)\n",
    "    return InfoString\n",
    "\n",
    "def ExtractBJWeather():\n",
    "    BJWeatherExcel = xlsxwriter.Workbook('北京天气爬虫.xlsx')\n",
    "    url = 'http://www.tianqihoubao.com/lishi/beijing.html'\n",
    "    head = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.106 Safari/537.36','Connection': 'close'}\n",
    "    html = requests.get(url, headers = head)\n",
    "    bsObj = BeautifulSoup(html.content, 'lxml') \n",
    "\n",
    "    OneYearWeather = bsObj.find_all('div', class_ = \"box pcity\")  \n",
    "    Year = 2010\n",
    "    for AQuarter in OneYearWeather:  \n",
    "        Year += 1\n",
    "        if Year <= 2020: \n",
    "            WorkSheet = BJWeatherExcel.add_worksheet(str(Year))\n",
    "            row = 0\n",
    "            col = 0\n",
    "            WorkSheet.write(row, col, '日期')  \n",
    "            WorkSheet.write(row, col+1, '天气状况')\n",
    "            WorkSheet.write(row, col+2, '气温')\n",
    "            WorkSheet.write(row, col+3, '风力风向')       \n",
    "            AQuarterData = AQuarter.find_all('ul')  \n",
    "            for AMonth in AQuarterData:  # 一个季度内遍历每个月的数据\n",
    "                # print(AMonth)  \n",
    "                ThreeMonthLink = AMonth.find_all('a')  \n",
    "                for Link in ThreeMonthLink: \n",
    "                    AMonthLink = Link['href']\n",
    "                    if '/lish' in str(AMonthLink): \n",
    "                        AMonthurl = 'http://www.tianqihoubao.com' + AMonthLink  \n",
    "                    else: \n",
    "                        AMonthurl = 'http://www.tianqihoubao.com/lish' + AMonthLink\n",
    "                    try: \n",
    "                        AMonth_HTML = requests.get(AMonthurl, headers = head) \n",
    "                        AMonthObj = BeautifulSoup(AMonth_HTML.content, 'lxml')  \n",
    "                        AMonthData = AMonthObj.find_all('tr') \n",
    "                        i = 1\n",
    "                        for ADay in AMonthData: \n",
    "                            if i ==1:\n",
    "                                i = 2\n",
    "                                continue\n",
    "                            else:\n",
    "                                ALine = ADay.find_all('td') \n",
    "                                col = 0 \n",
    "                                row += 1\n",
    "                                for info in ALine: # 遍历行的四项信息\n",
    "                                    AnInfo = str(info.get_text())  # 获取行数据下的文本内容\n",
    "                                    AnInfo = CleanData(AnInfo) # 除去数据中的\\n、\\r、空格，以及变换日期格式\n",
    "                                    WorkSheet.write(row, col, AnInfo) # 数据写入表格\n",
    "                                    col += 1 # 列增加\n",
    "                    except:  # 请求过快时，按5s休息处理\n",
    "                        print(\"requests speed so high,need sleep!\")\n",
    "                        time.sleep(5)\n",
    "                        print(\"continue...\")\n",
    "                        continue\n",
    "            print('...爬完 '+ str(Year) + ' 年...')\n",
    "        else:\n",
    "            break\n",
    "    BJWeatherExcel.close()\n",
    "    print('---------------------end----------------')\n",
    "    return \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    ExtractBJWeather()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished scraping data for 2011...\n",
      "---------------------Scraping Complete----------------\n"
     ]
    }
   ],
   "source": [
    "def CleanData(InfoString):\n",
    "    InfoString = InfoString.replace('\\n', '',)  \n",
    "    InfoString = InfoString.replace('\\r', '',)  \n",
    "    InfoString = InfoString.replace(' ', '',)\n",
    "    InfoString = InfoString.replace('年', '-', 1)  \n",
    "    InfoString = InfoString.replace('月', '-', 1)\n",
    "    InfoString = InfoString.replace('日', '', 1)\n",
    "    return InfoString\n",
    "\n",
    "def ExtractBJWeather():\n",
    "    # Scrape Beijing weather data\n",
    "    output_file = 'Beijing_Weather_Data.csv'\n",
    "    with open(output_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Date', 'Weather Condition', 'Temperature', 'Wind Force and Direction'])  # CSV headers\n",
    "\n",
    "        url = 'http://www.tianqihoubao.com/lishi/beijing.html'\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.106 Safari/537.36',\n",
    "            'Connection': 'close'\n",
    "        }\n",
    "        html = requests.get(url, headers=headers)\n",
    "        html.encoding = 'gb2312'  # Correctly set the encoding to match the website\n",
    "        bsObj = BeautifulSoup(html.text, 'lxml')  # Use text instead of content with the correct encoding\n",
    "\n",
    "        year_weather_data = bsObj.find_all('div', class_=\"box pcity\")  # Weather data for all years\n",
    "        year = 2010\n",
    "        for quarter_data in year_weather_data:  # Iterate through quarterly data\n",
    "            year += 1\n",
    "            if year <= 2011:  # Restrict to data up to 2020\n",
    "                quarter_list = quarter_data.find_all('ul')  # Quarterly data\n",
    "                for month_data in quarter_list:  # Iterate through months in a quarter\n",
    "                    month_links = month_data.find_all('a')  # Extract links for each month\n",
    "                    for link in month_links:\n",
    "                        month_url = urljoin('http://www.tianqihoubao.com', link['href'])\n",
    "                        try:\n",
    "                            month_html = requests.get(month_url, headers=headers)\n",
    "                            month_html.encoding = 'gb2312'  # Correct encoding\n",
    "                            month_bsObj = BeautifulSoup(month_html.text, 'lxml')\n",
    "                            month_rows = month_bsObj.find_all('tr')  # Rows of monthly data\n",
    "                            for i, day_row in enumerate(month_rows):\n",
    "                                if i == 0:  # Skip the header row\n",
    "                                    continue\n",
    "                                day_data = [CleanData(td.get_text()) for td in day_row.find_all('td')]\n",
    "                                if len(day_data) == 4:\n",
    "                                    writer.writerow(day_data)  # Write data to CSV file\n",
    "                        except Exception as e:  # Handle request rate limit\n",
    "                            print(\"Request rate too high, sleeping for 5 seconds...\")\n",
    "                            time.sleep(5)\n",
    "                            print(\"Retrying...\")\n",
    "                            continue\n",
    "                print(f'Finished scraping data for {year}...')\n",
    "            else:\n",
    "                break\n",
    "    print('---------------------Scraping Complete----------------')\n",
    "    return\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    ExtractBJWeather()   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
