{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (2.32.3)\n",
      "Collecting bs4\n",
      "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
      "Collecting lxml\n",
      "  Downloading lxml-5.3.0-cp312-cp312-macosx_10_9_universal2.whl.metadata (3.8 kB)\n",
      "Collecting xlsxwriter\n",
      "  Downloading XlsxWriter-3.2.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests) (2024.2.2)\n",
      "Requirement already satisfied: beautifulsoup4 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from bs4) (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from beautifulsoup4->bs4) (2.5)\n",
      "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
      "Downloading lxml-5.3.0-cp312-cp312-macosx_10_9_universal2.whl (8.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading XlsxWriter-3.2.0-py3-none-any.whl (159 kB)\n",
      "Installing collected packages: xlsxwriter, lxml, bs4\n",
      "Successfully installed bs4-0.0.2 lxml-5.3.0 xlsxwriter-3.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install requests bs4 lxml xlsxwriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from urllib.parse import urljoin\n",
    "import pandas as pd\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished scraping data for 2011...\n",
      "Finished scraping data for 2012...\n",
      "Finished scraping data for 2013...\n",
      "Finished scraping data for 2014...\n",
      "Finished scraping data for 2015...\n",
      "Finished scraping data for 2016...\n",
      "Finished scraping data for 2017...\n",
      "Finished scraping data for 2018...\n",
      "Finished scraping data for 2019...\n",
      "Finished scraping data for 2020...\n",
      "Finished scraping data for 2021...\n",
      "Scraping data for beijing completed!\n",
      "Finished scraping data for 2011...\n",
      "Finished scraping data for 2012...\n",
      "Finished scraping data for 2013...\n",
      "Finished scraping data for 2014...\n",
      "Finished scraping data for 2015...\n",
      "Finished scraping data for 2016...\n",
      "Finished scraping data for 2017...\n",
      "Finished scraping data for 2018...\n",
      "Finished scraping data for 2019...\n",
      "Finished scraping data for 2020...\n",
      "Finished scraping data for 2021...\n",
      "Scraping data for shanghai completed!\n",
      "Finished scraping data for 2011...\n",
      "Finished scraping data for 2012...\n",
      "Finished scraping data for 2013...\n",
      "Finished scraping data for 2014...\n",
      "Finished scraping data for 2015...\n",
      "Finished scraping data for 2016...\n",
      "Finished scraping data for 2017...\n",
      "Finished scraping data for 2018...\n",
      "Finished scraping data for 2019...\n",
      "Finished scraping data for 2020...\n",
      "Finished scraping data for 2021...\n",
      "Scraping data for lasa completed!\n",
      "Finished scraping data for 2011...\n",
      "Finished scraping data for 2012...\n",
      "Finished scraping data for 2013...\n",
      "Finished scraping data for 2014...\n",
      "Finished scraping data for 2015...\n",
      "Finished scraping data for 2016...\n",
      "Finished scraping data for 2017...\n",
      "Finished scraping data for 2018...\n",
      "Finished scraping data for 2019...\n",
      "Finished scraping data for 2020...\n",
      "Finished scraping data for 2021...\n",
      "Scraping data for haikou completed!\n"
     ]
    }
   ],
   "source": [
    "def CleanData(InfoString):\n",
    "    \"\"\"Clean the data by removing unwanted characters\"\"\"\n",
    "    InfoString = InfoString.replace('\\n', '',)  \n",
    "    InfoString = InfoString.replace('\\r', '',)  \n",
    "    InfoString = InfoString.replace(' ', '',)\n",
    "    InfoString = InfoString.replace('年', '-', 1)  \n",
    "    InfoString = InfoString.replace('月', '-', 1)\n",
    "    InfoString = InfoString.replace('日', '', 1)\n",
    "    return InfoString\n",
    "\n",
    "def ExtractWeatherForCity(city):\n",
    "    \"\"\"Scrape weather data for the specified city and append to the same CSV file\"\"\"\n",
    "    # Define the base URL for the weather history page for different cities\n",
    "    url = f'http://www.tianqihoubao.com/lishi/{city}.html'\n",
    "    \n",
    "    # Open the same CSV file to append data from different cities\n",
    "    output_file = 'Weather_Data_All_Cities.csv'\n",
    "    with open(output_file, mode='a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        \n",
    "        # If it's the first city, write the header\n",
    "        if file.tell() == 0:  # Check if file is empty, i.e., first write\n",
    "            writer.writerow(['Date', 'City', 'Weather Condition', 'Temperature', 'Wind Force and Direction'])  # CSV headers\n",
    "        \n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.106 Safari/537.36',\n",
    "            'Connection': 'close'\n",
    "        }\n",
    "        html = requests.get(url, headers=headers)\n",
    "        html.encoding = 'gb2312'  # Correctly set the encoding to match the website\n",
    "        bsObj = BeautifulSoup(html.text, 'lxml')\n",
    "\n",
    "        # Find the weather data for the city\n",
    "        year_weather_data = bsObj.find_all('div', class_=\"box pcity\")  # Weather data for all years\n",
    "        year = 2010\n",
    "        for quarter_data in year_weather_data:  # Iterate through quarterly data\n",
    "            year += 1\n",
    "            if year <= 2021:  # Restrict to data up to 2020\n",
    "                quarter_list = quarter_data.find_all('ul')  # Quarterly data\n",
    "                for month_data in quarter_list:  # Iterate through months in a quarter\n",
    "                    month_links = month_data.find_all('a')  # Extract links for each month\n",
    "                    for link in month_links:\n",
    "                        month_url = urljoin('http://www.tianqihoubao.com', link['href'])\n",
    "                        try:\n",
    "                            month_html = requests.get(month_url, headers=headers)\n",
    "                            month_html.encoding = 'gb2312'  # Correct encoding\n",
    "                            month_bsObj = BeautifulSoup(month_html.text, 'lxml')\n",
    "                            month_rows = month_bsObj.find_all('tr')  # Rows of monthly data\n",
    "                            for i, day_row in enumerate(month_rows):\n",
    "                                if i == 0:  # Skip the header row\n",
    "                                    continue\n",
    "                                day_data = [CleanData(td.get_text()) for td in day_row.find_all('td')]\n",
    "                                if len(day_data) == 4:\n",
    "                                    day_data.insert(1, city)  # Insert city name at the second column\n",
    "                                    writer.writerow(day_data)  # Write data to CSV file\n",
    "                        except Exception as e:  # Handle request rate limit\n",
    "                            print(f\"Error scraping {city} data for month: {link['href']}, retrying...\")\n",
    "                            time.sleep(5)\n",
    "                            continue\n",
    "                print(f'Finished scraping data for {year}...')\n",
    "            else:\n",
    "                break\n",
    "    print(f'Scraping data for {city} completed!')\n",
    "    return\n",
    "\n",
    "# Example: Scrape weather data for Beijing, Shanghai, and Guangzhou\n",
    "ExtractWeatherForCity('beijing')\n",
    "ExtractWeatherForCity('shanghai')\n",
    "ExtractWeatherForCity('lasa')\n",
    "ExtractWeatherForCity('haikou')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Date     City Weather Condition Temperature Wind Force and Direction  \\\n",
      "0 2011-01-01  beijing               晴/晴      0℃/-9℃        无持续风向≤3级/无持续风向≤3级   \n",
      "1 2011-01-02  beijing              多云/阴     -2℃/-7℃        无持续风向≤3级/无持续风向≤3级   \n",
      "2 2011-01-03  beijing               晴/晴      1℃/-8℃          北风3-4级/无持续风向≤3级   \n",
      "3 2011-01-04  beijing               晴/晴    -1℃/-11℃        无持续风向≤3级/无持续风向≤3级   \n",
      "4 2011-01-05  beijing               晴/晴     -1℃/-8℃            北风4-5级/北风3-4级   \n",
      "\n",
      "  Day Weather Night Weather  Day Temp  Night Temp Day Wind Direction  \\\n",
      "0           晴             晴       0.0        -9.0              无持续风向   \n",
      "1          多云             阴      -2.0        -7.0              无持续风向   \n",
      "2           晴             晴       1.0        -8.0                 北风   \n",
      "3           晴             晴      -1.0       -11.0              无持续风向   \n",
      "4           晴             晴      -1.0        -8.0                 北风   \n",
      "\n",
      "  Night Wind Direction Day Wind Force Night Wind Force  \n",
      "0                无持续风向            ≤3级              ≤3级  \n",
      "1                无持续风向            ≤3级              ≤3级  \n",
      "2                无持续风向           3-4级              ≤3级  \n",
      "3                无持续风向            ≤3级              ≤3级  \n",
      "4                   北风           4-5级             3-4级  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = pd.read_csv('Weather_Data_All_Cities.csv')\n",
    "data['Date'] = pd.to_datetime(data['Date'], format='%Y-%m-%d')\n",
    "data[['Day Weather', 'Night Weather']] = data['Weather Condition'].str.split('/', expand=True)\n",
    "\n",
    "data[['Day Temp', 'Night Temp']] = data['Temperature'].str.split('/', expand=True)\n",
    "\n",
    "data['Day Temp'] = data['Day Temp'].str.replace('℃', '').replace('', pd.NA)\n",
    "data['Night Temp'] = data['Night Temp'].str.replace('℃', '').replace('', pd.NA)\n",
    "\n",
    "data['Day Temp'] = pd.to_numeric(data['Day Temp'], errors='coerce')  # 'coerce'会将无法转换的值设为NaN\n",
    "data['Night Temp'] = pd.to_numeric(data['Night Temp'], errors='coerce')\n",
    "\n",
    "wind_force_pattern = r'(\\d+-\\d+级|\\d+级|≤\\d+级)'\n",
    "\n",
    "# 风向的正则：匹配包含'风'的字符串（例如：北风、东南风、无持续风向等）\n",
    "wind_direction_pattern = r'([东南西北]+风|无持续风向)'\n",
    "\n",
    "# 分离风力和风向信息\n",
    "def extract_wind_info(wind_data):\n",
    "    # 使用斜杠来分开白天和夜间的风力风向数据\n",
    "    if pd.isnull(wind_data):\n",
    "        return ['Unknown', 'Unknown', 'Unknown', 'Unknown']\n",
    "\n",
    "    parts = wind_data.split('/')  # 通过斜杠分开白天和夜间的数据\n",
    "    if len(parts) == 2:  # 如果有两部分数据（即白天和夜间）\n",
    "        day_data = parts[0]\n",
    "        night_data = parts[1]\n",
    "    else:\n",
    "        day_data = parts[0]\n",
    "        night_data = '无持续风向'  # 默认值，如果夜间没有数据\n",
    "\n",
    "    # 提取白天和夜间风力与风向\n",
    "    day_wind_direction = re.search(wind_direction_pattern, day_data)\n",
    "    night_wind_direction = re.search(wind_direction_pattern, night_data)\n",
    "    day_wind_force = re.search(wind_force_pattern, day_data)\n",
    "    night_wind_force = re.search(wind_force_pattern, night_data)\n",
    "\n",
    "    day_wind_direction = day_wind_direction.group(0) if day_wind_direction else 'Unknown'\n",
    "    night_wind_direction = night_wind_direction.group(0) if night_wind_direction else 'Unknown'\n",
    "    day_wind_force = day_wind_force.group(0) if day_wind_force else 'Unknown'\n",
    "    night_wind_force = night_wind_force.group(0) if night_wind_force else 'Unknown'\n",
    "\n",
    "    return [day_wind_direction, night_wind_direction, day_wind_force, night_wind_force]\n",
    "\n",
    "# 通过apply方法将提取操作应用到每一行的'Wind Force and Direction'列\n",
    "data[['Day Wind Direction', 'Night Wind Direction', 'Day Wind Force', 'Night Wind Force']] = data['Wind Force and Direction'].apply(\n",
    "    extract_wind_info).apply(pd.Series)\n",
    "\n",
    "# 查看处理后的数据\n",
    "print(data.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
